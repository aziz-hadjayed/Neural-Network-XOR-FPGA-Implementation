# Neural Network XOR - FPGA Implementation

![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![VHDL](https://img.shields.io/badge/VHDL-543978?style=for-the-badge&logo=vhdl&logoColor=white)
![FPGA](https://img.shields.io/badge/FPGA-0071C5?style=for-the-badge&logo=xilinx&logoColor=white)

ImplÃ©mentation complÃ¨te d'un rÃ©seau de neurones artificiel pour rÃ©soudre le problÃ¨me classique XOR : entraÃ®nement logiciel avec TensorFlow/Keras et dÃ©ploiement matÃ©riel sur FPGA en VHDL. Ce projet dÃ©montre le pipeline complet du logiciel au matÃ©riel pour l'intelligence artificielle embarquÃ©e.

## Table des matiÃ¨res

- [Vue d'ensemble](#-vue-densemble)
- [ProblÃ¨me XOR](#-problÃ¨me-xor)
- [Architecture du rÃ©seau](#-architecture-du-rÃ©seau)
- [Partie logicielle (Python)](#-partie-logicielle-python)
- [Partie matÃ©rielle (VHDL)](#-partie-matÃ©rielle-vhdl)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [RÃ©sultats](#-rÃ©sultats)
- [Structure du projet](#-structure-du-projet)
- [Licence](#-licence)

## Vue d'ensemble

Ce projet illustre l'implÃ©mentation d'un rÃ©seau de neurones artificiel Ã  deux niveaux :

1. **Niveau logiciel** : EntraÃ®nement avec TensorFlow/Keras (Python)
   - Construction du modÃ¨le
   - EntraÃ®nement sur la fonction XOR
   - Visualisation des performances (loss, accuracy)
   - Affichage des frontiÃ¨res de dÃ©cision
   - Extraction des poids et biais

2. **Niveau matÃ©riel** : DÃ©ploiement sur FPGA (VHDL)
   - ImplÃ©mentation matÃ©rielle du rÃ©seau entraÃ®nÃ©
   - Calcul en virgule fixe
   - Simulation avec ModelSim
   - SynthÃ¨se pour FPGA

### Pourquoi XOR ?

Le **XOR (OU exclusif)** est un problÃ¨me classique en apprentissage automatique car :
- âŒ Non linÃ©airement sÃ©parable (un perceptron simple ne peut pas le rÃ©soudre)
- âœ… NÃ©cessite au moins une couche cachÃ©e
- âœ… Parfait pour dÃ©montrer l'apprentissage de fonctions non linÃ©aires
- âœ… Compact (4 exemples seulement) mais reprÃ©sentatif

## ProblÃ¨me XOR

### Table de vÃ©ritÃ©

| EntrÃ©e 1 | EntrÃ©e 2 | Sortie XOR |
|----------|----------|------------|
| 0        | 0        | 0          |
| 0        | 1        | 1          |
| 1        | 0        | 1          |
| 1        | 1        | 0          |

### ReprÃ©sentation graphique

```
    EntrÃ©e 2
      ^
      |
    1 +     (0,1)â—         (1,1)â—
      |       [1]           [0]
      |
  0.5 +
      |
      |
    0 +     (0,0)â—         (1,0)â—
      |       [0]           [1]
      |
      +----+----+----+----+----+---> EntrÃ©e 1
           0   0.5   1

â— Points bleus  : Sortie = 0
â— Points rouges : Sortie = 1
```

Les points de mÃªme classe ne sont **pas** linÃ©airement sÃ©parables !

## Architecture du rÃ©seau

### Structure

```
        Input Layer        Hidden Layer         Output Layer
           (2)                 (2)                  (1)
       
       â”Œâ”€â”€â”€â”                â”Œâ”€â”€â”€â”              
   x1 â”€â”¤   â”œâ”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”        
       â””â”€â”€â”€â”˜   â”‚    w11,w21 â””â”€â”€â”€â”˜     â”‚        
               â”‚                      â”‚   w1   â”Œâ”€â”€â”€â”
               â”‚            â”Œâ”€â”€â”€â”     â”œâ”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€ Å·
               â”‚    w12,w22 â”‚   â”‚     â”‚        â””â”€â”€â”€â”˜
       â”Œâ”€â”€â”€â”   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”˜         sigmoid
   x2 â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â””â”€â”€â”€â”˜      w2
       â””â”€â”€â”€â”˜                sigmoid
                            + bias
```

### SpÃ©cifications

| Couche | Neurones | Activation | ParamÃ¨tres |
|--------|----------|------------|------------|
| Input  | 2        | -          | 0          |
| Hidden | 2        | Sigmoid    | 6 (4W + 2b)|
| Output | 1        | Sigmoid    | 3 (2W + 1b)|
| **Total** | **5** | -       | **9**      |

**Fonction Sigmoid :**
```
Ïƒ(x) = 1 / (1 + e^(-x))
```

## Partie logicielle (Python)

### FonctionnalitÃ©s

Le script Python `neuron_xor.py` implÃ©mente :

1. **Construction du modÃ¨le**
   ```python
   model = tf.keras.Sequential([
       tf.keras.layers.Dense(2, activation='sigmoid', input_shape=(2,)),
       tf.keras.layers.Dense(1, activation='sigmoid')
   ])
   ```

2. **EntraÃ®nement**
   - Optimiseur : Adam (learning_rate=0.1)
   - Loss : Binary Crossentropy
   - MÃ©trique : Accuracy
   - Epochs : 150
   - DonnÃ©es : 4 exemples XOR

3. **Visualisations**
   - ğŸ“‰ Courbe de loss (entraÃ®nement + validation)
   - ğŸ“ˆ Courbe d'accuracy (entraÃ®nement + validation)
   - ğŸ¨ FrontiÃ¨re de dÃ©cision (contour plot 2D)

4. **Analyse du modÃ¨le**
   - Affichage des prÃ©dictions
   - Extraction des poids (W) et biais (b)
   - CaractÃ©ristiques dÃ©taillÃ©es de chaque couche

5. **FrontiÃ¨re de dÃ©cision**
   - Grille de 100Ã—100 points
   - PrÃ©diction pour chaque point
   - Visualisation avec colormap (RdYlBu)

### Exemple de sortie

```
RÃ©sumÃ© du modÃ¨le :
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
dense (Dense)               (None, 2)                 6         
dense_1 (Dense)             (None, 1)                 3         
=================================================================
Total params: 9

PrÃ©dictions sur la table XOR :
EntrÃ©e: [0. 0.]  â†’  Vrai: 0  |  PrÃ©dit: 0.0234  |  Arrondi: 0
EntrÃ©e: [0. 1.]  â†’  Vrai: 1  |  PrÃ©dit: 0.9812  |  Arrondi: 1
EntrÃ©e: [1. 0.]  â†’  Vrai: 1  |  PrÃ©dit: 0.9823  |  Arrondi: 1
EntrÃ©e: [1. 1.]  â†’  Vrai: 0  |  PrÃ©dit: 0.0187  |  Arrondi: 0
```

## Partie matÃ©rielle (VHDL)

### Objectif

ImplÃ©menter le rÃ©seau de neurones entraÃ®nÃ© **directement en matÃ©riel** sur FPGA.

### DÃ©fis de l'implÃ©mentation matÃ©rielle

1. **Virgule fixe vs Virgule flottante**
   - Python : float32 (virgule flottante)
   - FPGA : virgule fixe (ex: Q8.8, Q16.16)
   - NÃ©cessite quantification des poids

2. **Fonction Sigmoid**
   - Exponentielles coÃ»teuses en matÃ©riel
   - Solutions : LUT (Look-Up Table), approximation polynomiale, CORDIC

3. **Pipeline et parallÃ©lisme**
   - Calculs parallÃ¨les des neurones
   - Optimisation du dÃ©bit (throughput)

### Architecture VHDL proposÃ©e

```vhdl
entity xor_neural_net is
    Port (
        clk     : in  std_logic;
        rst     : in  std_logic;
        x1      : in  std_logic_vector(15 downto 0);  -- Q8.8
        x2      : in  std_logic_vector(15 downto 0);  -- Q8.8
        valid_in: in  std_logic;
        y_out   : out std_logic_vector(15 downto 0);  -- Q8.8
        valid_out: out std_logic
    );
end xor_neural_net;
```

### Modules VHDL

| Module | Description |
|--------|-------------|
| `neuron.vhd` | Neurone unique (MAC + activation) |
| `sigmoid_lut.vhd` | Table de correspondance sigmoid |
| `hidden_layer.vhd` | Couche cachÃ©e (2 neurones) |
| `output_layer.vhd` | Couche de sortie (1 neurone) |
| `xor_neural_net.vhd` | Top-level entity |
| `tb_xor_neural_net.vhd` | Testbench ModelSim |

### Simulation ModelSim

Le testbench vÃ©rifie les 4 cas XOR :

```vhdl
-- Testbench stimulus
process
begin
    -- Test case 1: (0, 0) â†’ 0
    x1 <= x"0000"; x2 <= x"0000"; valid_in <= '1';
    wait for 10 ns;
    
    -- Test case 2: (0, 1) â†’ 1
    x1 <= x"0000"; x2 <= x"0100"; valid_in <= '1';
    wait for 10 ns;
    
    -- Test case 3: (1, 0) â†’ 1
    x1 <= x"0100"; x2 <= x"0000"; valid_in <= '1';
    wait for 10 ns;
    
    -- Test case 4: (1, 1) â†’ 0
    x1 <= x"0100"; x2 <= x"0100"; valid_in <= '1';
    wait for 10 ns;
    
    wait;
end process;
```

### Quantification des poids

**Exemple de conversion Python â†’ VHDL :**

```python
# Python (float32)
w1 = 4.8532

# VHDL (Q8.8 - 8 bits entier, 8 bits fractionnaire)
w1_fixed = int(4.8532 * 256)  # = 1242 = 0x04DA
```

En VHDL :
```vhdl
constant W1 : signed(15 downto 0) := x"04DA";  -- 4.8532 en Q8.8
```

## Installation

### PrÃ©requis

**Partie Python :**
- Python 3.8+
- TensorFlow 2.x
- NumPy
- Matplotlib

**Partie VHDL :**
- ModelSim (Intel/Mentor Graphics)

### Installation Python

```bash
# Cloner le repository
git clone https://github.com/votre-username/Neural-Network-XOR-FPGA-Implementation.git
cd Neural-Network-XOR-FPGA-Implementation

# CrÃ©er un environnement virtuel (optionnel)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

**Fichier `requirements.txt` :**
```
tensorflow>=2.10.0
numpy>=1.23.0
matplotlib>=3.5.0
```

### Installation ModelSim

1. TÃ©lÃ©charger [ModelSim-Intel FPGA Edition](https://www.intel.com/content/www/us/en/software-kit/750666/modelsim-intel-fpgas-standard-edition-software-version-20-1-1.html)
2. Installer et configurer le PATH
3. VÃ©rifier l'installation :
   ```bash
   vsim -version
   ```

## Utilisation

### 1. EntraÃ®nement du rÃ©seau (Python)

```bash
python neuron_xor.py
```

**Sorties :**
- RÃ©sumÃ© du modÃ¨le dans le terminal
- Graphiques de loss et accuracy (matplotlib)
- FrontiÃ¨re de dÃ©cision
- Poids et biais extraits

### 2. Extraire les poids pour VHDL

Les poids sont affichÃ©s Ã  la fin de l'exÃ©cution :

```
Poids et biais appris :
Poids (w) : [ 4.8532  4.9123 -7.1234 -7.0987]
Biais (b) : [-2.1234 -3.4567]
```

Convertir en virgule fixe :
```python
import numpy as np

# Format Q8.8 (8 bits entier, 8 bits fractionnaire)
scale = 256

weights_float = [4.8532, 4.9123, -7.1234, -7.0987]
weights_fixed = [int(w * scale) for w in weights_float]

print("Poids en Q8.8 (hex):")
for i, wf in enumerate(weights_fixed):
    print(f"W{i} : 0x{wf & 0xFFFF:04X}")
```

### 3. Simulation VHDL (ModelSim)

```bash
cd vhdl/

# Compiler les fichiers VHDL
vlib work
vcom neuron.vhd
vcom sigmoid_lut.vhd
vcom hidden_layer.vhd
vcom output_layer.vhd
vcom xor_neural_net.vhd
vcom tb_xor_neural_net.vhd

# Lancer la simulation
vsim -do sim.do tb_xor_neural_net

# Ou en mode GUI
vsim -gui work.tb_xor_neural_net
```

**Script `sim.do` :**
```tcl
# Ajouter les signaux Ã  la fenÃªtre wave
add wave -radix hexadecimal /tb_xor_neural_net/*

# ExÃ©cuter la simulation
run 100 ns

# Zoom sur les signaux
wave zoom full
```

### 4. SynthÃ¨se FPGA

#### Pour Intel/Altera (Quartus)

```bash
quartus_sh --flow compile xor_neural_net.qpf
```

#### Pour Xilinx (Vivado)

```bash
vivado -mode batch -source build.tcl
```

## RÃ©sultats

### Performances Python

| MÃ©trique | Valeur |
|----------|--------|
| Loss finale (entraÃ®nement) | 0.0324 |
| Loss finale (validation) | 0.0319 |
| Accuracy finale | **100%** (4/4) |
| Epochs pour convergence | ~80 |
| Temps d'entraÃ®nement | < 5 secondes |
| GPU utilisÃ© | NVIDIA GeForce RTX 4060 Laptop |

### PrÃ©dictions finales

| EntrÃ©e (x1, x2) | Cible | PrÃ©diction | Arrondi | âœ“ |
|-----------------|-------|------------|---------|---|
| (0, 0) | 0 | 0.0275 | 0 | âœ… |
| (0, 1) | 1 | 0.9754 | 1 | âœ… |
| (1, 0) | 1 | 0.9517 | 1 | âœ… |
| (1, 1) | 0 | 0.0252 | 0 | âœ… |

**Accuracy : 100%** - Le rÃ©seau a parfaitement appris la fonction XOR !

### Poids et biais appris

#### Couche cachÃ©e (Layer 0: dense)

**Matrice des poids W (2Ã—2) :**
```
[[-9.213075  -6.8804436]
 [ 9.11554    7.3025427]]
```

**Vecteur des biais b (2,) :**
```
[-4.864452   3.2528257]
```

#### Couche de sortie (Layer 1: dense_1)

**Matrice des poids W (2Ã—1) :**
```
[[ 7.6743217]
 [-7.0485606]]
```

**Vecteur des biais b (1,) :**
```
[3.1635947]
```

### Courbes d'entraÃ®nement

#### Ã‰volution de la Loss

![Loss Curve](images/loss.png)

- **Loss d'entraÃ®nement** : Descend de ~0.8 Ã  ~0.03
- **Loss de validation** : Suit la mÃªme tendance
- **Convergence** : Vers l'epoch 80
- Pas d'overfitting (courbes superposÃ©es)

#### Ã‰volution de l'Accuracy

![Accuracy Curve](images/accuracy.png)

- **Accuracy d'entraÃ®nement** : Atteint 100% vers l'epoch 80
- **Accuracy de validation** : Identique (100%)
- Apprentissage progressif avec quelques oscillations initiales

### FrontiÃ¨re de dÃ©cision

![Decision Boundary](images/decision_boundary.png)

La visualisation montre clairement la **sÃ©paration non linÃ©aire** des classes :

```
    EntrÃ©e 2
      ^
      |
    1 +  ğŸ”µ(0,1)         ğŸ”´(1,1)
      |    [1]             [0]
      |      \           /
  0.5 +       \  Zone  /
      |        \ Orange/
      |         \  /  
    0 +  ğŸ”´(0,0)  X  ğŸ”µ(1,0)
      |    [0]    /\    [1]
      |          /  \
      +â”€â”€â”€â”€â”€â”€â”€â”€â”€+â”€â”€â”€â”€+â”€â”€â”€â”€â”€> EntrÃ©e 1
               0   0.5   1
```

**LÃ©gende :**
- ğŸ”µ Zone bleue : Sortie prÃ©dite = 0 (classe 0)
- ğŸ”´ Zone orange : Sortie prÃ©dite = 1 (classe 1)
- Points noirs : DonnÃ©es d'entraÃ®nement
- FrontiÃ¨re : Courbe diagonale sÃ©parant les deux rÃ©gions

La frontiÃ¨re de dÃ©cision est une **courbe non linÃ©aire** qui sÃ©pare parfaitement les points :
- **(0,0)** et **(1,1)** â†’ Classe 0 (zone bleue)
- **(0,1)** et **(1,0)** â†’ Classe 1 (zone orange)

### Performances FPGA

| MÃ©trique | Valeur (estimation) |
|----------|---------------------|
| FrÃ©quence max | ~100 MHz |
| Latence | 3-5 cycles d'horloge |
| LUTs utilisÃ©es | ~500-1000 |
| Registres | ~200-400 |
| DSP blocks | 0 (MAC en logique) ou 4-6 |
| PrÃ©cision (Q8.8) | ~99% accuracy |

*Valeurs dÃ©pendant du FPGA cible et des optimisations*

## Concepts clÃ©s

### 1. Perceptron multicouche (MLP)

Un rÃ©seau de neurones feedforward avec :
- Couche d'entrÃ©e
- Au moins une couche cachÃ©e
- Couche de sortie
- Connexions complÃ¨tes entre couches adjacentes

### 2. RÃ©tropropagation (Backpropagation)

Algorithme d'entraÃ®nement qui :
1. Calcule l'erreur de sortie
2. Propage l'erreur vers l'arriÃ¨re
3. Ajuste les poids proportionnellement Ã  l'erreur

### 3. Fonction d'activation Sigmoid

```
Ïƒ(x) = 1 / (1 + e^(-x))
```

PropriÃ©tÃ©s :
- Sortie dans [0, 1]
- DÃ©rivable (nÃ©cessaire pour backprop)
- Non linÃ©aire (permet d'apprendre des fonctions complexes)

### 4. Virgule fixe vs flottante

| Aspect | Virgule flottante | Virgule fixe |
|--------|-------------------|--------------|
| PrÃ©cision | Ã‰levÃ©e | LimitÃ©e |
| Plage | Large (-âˆ, +âˆ) | Fixe |
| MatÃ©riel | Complexe | Simple |
| Vitesse FPGA | Lente | Rapide |
| Ressources | Nombreuses | Peu |

## Applications pÃ©dagogiques

Ce projet est idÃ©al pour :

âœ… **Cours d'Intelligence Artificielle**
- Introduction aux rÃ©seaux de neurones
- Apprentissage supervisÃ©
- Classification binaire

âœ… **Cours d'Architectures MatÃ©rielles**
- Conception numÃ©rique avec VHDL
- ImplÃ©mentation d'algorithmes sur FPGA
- Optimisation matÃ©rielle

**Projets Ã©tudiants**
- Pont entre IA et Ã©lectronique numÃ©rique
- DÃ©monstration pratique du pipeline logiciel â†’ matÃ©riel
- Base pour des rÃ©seaux plus complexes


## DÃ©pannage

### ProblÃ¨me : Loss ne converge pas

**Solutions :**
- Diminuer le learning rate (ex: 0.01 au lieu de 0.1)
- Augmenter le nombre d'epochs
- Changer l'optimiseur (SGD au lieu d'Adam)
- VÃ©rifier les donnÃ©es d'entrÃ©e (normalisÃ©es entre 0 et 1)

### ProblÃ¨me : ModelSim ne compile pas

**VÃ©rifications :**
- Version VHDL correcte (VHDL-2008 si nÃ©cessaire)
- Ordre de compilation des fichiers
- Syntaxe des librairies (ieee.std_logic_1164.all)

```bash
# VÃ©rifier la version
vsim -version

# Compiler en VHDL-2008
vcom -2008 neuron.vhd
```

### ProblÃ¨me : PrÃ©cision rÃ©duite en FPGA

**Causes possibles :**
- Virgule fixe trop courte (essayer Q16.16)
- LUT sigmoid trop grossiÃ¨re (augmenter la rÃ©solution)
- Overflow dans les multiplications

**Solutions :**
```vhdl
-- Augmenter la prÃ©cision
constant FIXED_POINT_WIDTH : integer := 32;  -- au lieu de 16
constant FRAC_BITS : integer := 16;          -- au lieu de 8
```

## Auteur

**[Votre Nom]**
- GitHub: [@aziz-hadjayed](https://github.com/votre-username)
- Email: mohamedaziz.hadjayed@enicar.ucar.tn

## Remerciements

- TensorFlow et Keras pour le framework de deep learning
- Intel/Mentor Graphics pour ModelSim
- La communautÃ© FPGA pour les ressources et tutoriels

---

â­ **Si ce projet vous aide Ã  comprendre les rÃ©seaux de neurones et FPGA, n'hÃ©sitez pas Ã  lui donner une Ã©toile !**

**Questions ?** Ouvrir une [issue](https://github.com/aziz-hadjayed/Neural-Network-XOR-FPGA-Implementation/issues)
